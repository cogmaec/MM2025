<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CogMAEC Workshop</title>
    <link rel="icon" type="image/x-icon" href="assets/logo.png">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        :root {
            --accent-color: #007bff;
            --theme-color: #c41230;
        }
        
        .hero {
            position: relative;
            background: linear-gradient(rgba(232, 241, 255, 0.85), rgba(232, 241, 255, 0.85)), url('assets/fig2.jpg');
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
        }

        .hero h1 {
            color: #c0392b;
        }

        .hero .subtitle {
            color: #e74c3c;
        }

        .hero .conference-info {
            color: #666;
        }

        .hero .description {
            color: #444;
        }

        .hero .description p {
            color: #444;
        }

        .hero .stay-updated {
            color: rgba(0, 0, 0, 0.8);
        }

        .hero .stay-updated a {
            color: #e74c3c;
            text-decoration: none;
        }

        .hero .stay-updated a:hover {
            color: #c0392b;
        }

        .hero .button {
            background-color: #c0392b;
            color: white;
        }

        .hero .button:hover {
            background-color: #a93226;
        }

        .accent-text {
            color: var(--accent-color) !important;
        }
        
        .theme-link {
            color: var(--theme-color) !important;
            text-decoration: none;
            transition: all 0.3s ease;
        }
        
        .theme-link:hover {
            text-decoration: underline;
        }

        .submission-button {
            display: inline-flex;
            align-items: center;
            gap: 10px;
            padding: 10px 20px;
            border: 2px solid var(--accent-color);
            border-radius: 5px;
            text-decoration: none;
            color: var(--theme-color) !important;
            background-color: #e8f1ff;
            transition: all 0.3s ease;
        }

        .submission-button:hover {
            background-color: #d9e8ff;
            text-decoration: none !important;
        }

        .submission-button .fa-paper-plane {
            color: var(--theme-color);
        }

        #submission a.theme-link {
            color: var(--theme-color) !important;
        }

        .email-btn {
            display: inline-block;
            padding: 2px 10px;
            font-size: 0.9em;
            color: #666;
            background-color: rgba(0, 0, 0, 0.05);
            border: 1px solid #ccc;
            border-radius: 4px;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        
        .email-btn:hover {
            background-color: rgba(0, 0, 0, 0.08);
        }

        .email-text {
            display: none;
            color: #444;
            font-style: italic;
            padding: 0 4px;
            text-decoration: underline;
            text-underline-offset: 2px;
        }

        .email-text.revealed {
            display: inline;
        }

        .speakers {
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 60px;
            margin: 40px 0;
        }

        .speaker {
            text-align: center;
            flex: 0 0 auto;
            max-width: 280px;
        }

        .speaker img {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            object-fit: cover;
            margin-bottom: 20px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
            box-sizing: border-box;
        }

        /* Make specific speaker photos look slightly smaller by adding inner padding */
        .speaker img.compact {
            padding: 8px;
            background-color: #fff;
            border: 1px solid #e9e9e9;
        }

        .speaker img:hover {
            transform: scale(1.05);
        }

        .speaker h3 {
            margin: 15px 0 10px;
            font-size: 1.3em;
        }

        .speaker h3 a {
            color: var(--theme-color, #c41230);
            text-decoration: none;
        }

        .speaker h3 a:hover {
            text-decoration: underline;
        }

        .speaker .institution {
            color: #666;
            font-size: 1.2em;
            margin: 0;
        }

        .schedule-section,
        .papers-section {
            padding: 60px 0;
        }

        .schedule-section .schedule-intro {
            margin-bottom: 24px;
            color: #444;
            line-height: 1.6;
        }

        .schedule-section .schedule-text {
            margin-bottom: 12px;
        }

        .schedule-section .schedule-meta {
            margin-top: 12px;
            font-weight: 600;
            color: #2c3e50;
            text-align: center;
        }

        .schedule-section .schedule-map-link {
            color: var(--theme-color);
            text-decoration: none;
        }

        .schedule-section .schedule-map-link:hover {
            text-decoration: underline;
        }

        .schedule-section .schedule-table-wrapper {
            overflow-x: auto;
            margin: 24px auto 0;
            width: 100%;
            max-width: none;
        }

        .schedule-section table {
            width: 100%;
            border-collapse: collapse;
            background: rgba(255, 255, 255, 0.95);
            box-shadow: 0 8px 24px rgba(0, 0, 0, 0.06);
            border-radius: 12px;
        }

        .schedule-section th,
        .schedule-section td {
            padding: 12px 18px;
            text-align: left;
            border-bottom: 1px solid rgba(0, 0, 0, 0.05);
            vertical-align: middle;
            line-height: 1.4;
        }

        .schedule-section th:nth-child(1),
        .schedule-section td:nth-child(1) {
            width: 20%;
        }

        .schedule-section th:nth-child(2),
        .schedule-section td:nth-child(2) {
            width: 48%;
        }

        .schedule-section th:nth-child(3),
        .schedule-section td:nth-child(3) {
            width: 32%;
        }

        .schedule-section thead {
            background-color: #f0f2f6;
        }

        .schedule-section th {
            font-weight: 700;
            color: #1f1f1f;
            letter-spacing: 0.02em;
        }

        .schedule-section .schedule-subhead th {
            background-color: #ffffff;
            font-size: 0.95em;
            font-weight: 600;
            color: #2c3e50;
            border-bottom: 1px solid rgba(0, 0, 0, 0.08);
        }

        .schedule-section tbody tr:nth-child(even) {
            background-color: rgba(245, 247, 250, 0.6);
        }

        .schedule-section .schedule-break td {
            text-align: center;
            background: #ffffff;
            font-weight: 400;
            font-size: 0.95em;
            padding: 12px 18px;
        }

        .schedule-section .time-slot {
            font-weight: 700;
            color: #1f1f1f;
            white-space: nowrap;
        }

        .schedule-section .session-title,
        .schedule-section .session-toggle {
            font-weight: 600;
            color: #222;
        }

        .schedule-section .session-title.keynote .session-highlight,
        .schedule-section .session-toggle.keynote .session-highlight {
            color: #1abc9c;
        }

        .schedule-section .session-title.oral .session-highlight,
        .schedule-section .session-toggle.oral .session-highlight {
            color: #f39c12;
        }

        .schedule-section .session-toggle {
            margin: 0;
        }

        .schedule-section .session-toggle summary {
            cursor: pointer;
            font-weight: 600;
            color: inherit;
            outline: none;
            line-height: 1.45;
        }

        .schedule-section .session-toggle summary::-webkit-details-marker {
            color: var(--theme-color);
        }

        .schedule-section .session-toggle summary:focus-visible {
            outline: 2px solid var(--accent-color);
            outline-offset: 4px;
        }

        .schedule-section .session-toggle[open] summary {
            color: #1f1f1f;
        }

        .schedule-section .session-toggle .session-toggle-content {
            margin-top: 10px;
            padding: 12px 16px;
            background: rgba(232, 241, 255, 0.6);
            border-left: 3px solid #1abc9c;
            border-radius: 6px;
            color: #3a3a3a;
            line-height: 1.6;
            font-size: 0.95em;
            font-weight: 400;
            text-align: justify;
            text-align-last: left;
        }

        .schedule-section .session-toggle .session-toggle-content p {
            text-align: justify;
            text-align-last: left;
            margin: 0;
        }

        .schedule-section .session-title.break {
            color: #7e8a97;
            font-style: italic;
        }

        .schedule-section .session-codes {
            font-size: 0.95em;
            color: #666;
        }

        .schedule-section .presenter {
            color: #333;
        }

        .schedule-section .presenter-affiliation {
            color: #555;
            font-size: 0.9em;
        }

        .schedule-section .note {
            font-style: italic;
            color: #666;
            margin-top: 12px;
        }

        .papers-table-wrapper {
            margin: 28px auto 0;
            width: 100%;
            max-width: none;
        }

        .papers-section table {
            width: 100%;
            border-collapse: collapse;
            background: rgba(255, 255, 255, 0.95);
            box-shadow: 0 6px 18px rgba(0, 0, 0, 0.05);
            border-radius: 12px;
            overflow: hidden;
        }

        .papers-section th,
        .papers-section td {
            padding: 6px 14px;
            border-bottom: 1px solid rgba(0, 0, 0, 0.05);
            text-align: left;
        }

        .papers-section tbody tr:nth-child(even) {
            background-color: rgba(245, 247, 250, 0.6);
        }

        .papers-section tbody tr:last-child td {
            border-bottom: none;
        }

        .papers-section .paper-title {
            font-weight: 700;
        }


        @media (max-width: 768px) {
            .speakers {
                flex-direction: column;
                align-items: center;
                gap: 40px;
            }
            
            .speaker {
                max-width: 100%;
            }

            .schedule-section th,
            .schedule-section td {
                padding: 10px 14px;
            }

            .schedule-section table {
                font-size: 0.95em;
            }

            .schedule-section .schedule-meta {
                font-size: 0.95em;
            }

            .schedule-section .schedule-table-wrapper,
            .papers-table-wrapper {
                width: 100%;
                max-width: none;
            }

            .papers-section table {
                font-size: 0.95em;
            }

            .schedule-section th:nth-child(1),
            .schedule-section td:nth-child(1),
            .schedule-section th:nth-child(2),
            .schedule-section td:nth-child(2),
            .schedule-section th:nth-child(3),
            .schedule-section td:nth-child(3) {
                width: auto;
            }
        }
    </style>
</head>
<body>
    <!-- <div class="background-container"></div> 新增背景容器 -->

    <div class="content-wrapper">
        <nav class="navbar">
            <div class="nav-links">
                <div class="logo">
                    <a href="#home">
                        <img src="./assets/logo.png" alt="Challenge Logo">
                    </a>
                </div>
                <div class="hamburger-menu">
                    <i class="fas fa-bars"></i>
                </div>
                <div class="nav-items">
                    <a href="#about">About</a>
                    <a href="#schedule">Schedule</a>
                    <a href="#accepted-papers">Accepted Papers</a>
                    <a href="#call-for-paper">Call for Papers</a>
                    <a href="#timeline">Timeline</a>
                    <a href="#submission">Submission</a>
                    <a href="#invited-speaker">Invited Speakers</a>
                    <a href="#organizers">Organizers</a>
                    <a href="#contact">Contact</a>
                    <a href="https://openreview.net/group?id=acmmm.org/ACMMM/2025/Workshop/CogMAEC" class="register-btn" target="_blank">Register</a>
                </div>
            </div>
        </nav>

        <section id="home" class="hero">
            <div class="container">
                <h1>The 1st CogMAEC Workshop</h1>
                <!-- <h1>CogMAEC @ <a href="https://acmmm2025.org/" target="_blank">ACM 2025</a> </h1> -->
                <h2 class="subtitle">Cognition-oriented Multimodal Affective and Empathetic Computing</h2>
                <div class="conference-info">
                    <p>27-31 October 2025 | Dublin, Ireland | <a href="https://acmmm2025.org/" target="_blank">ACM Multimedia 2025</a></p>
                </div>
                <div class="description">
                    <p>While multimodal systems excel at basic emotion recognition, they struggle to understand why we feel and how emotions evolve. This workshop pioneers cognitive AI that interprets human affect through multimodal context and causal reasoning.
                    Join us in redefining emotional intelligence for healthcare robots, empathetic chatbots, and beyond.</p>
                    
                    <p class="stay-updated">Stay updated: <a href="https://CogMAEC.github.io/MM2025" target="_blank">https://CogMAEC.github.io/MM2025</a></p>
                </div>
                <a href="#about" class="button">Learn More</a>
            </div>
        </section>

        <section id="about">
            <div class="container">
                <h2>About the CogMAEC</h2>
                <div class="about-content">
                    <p class="welcome-text">Welcome to the 1st CogMAEC Workshop, proudly co-located with <a href="https://acmmm2025.org/" target="_blank">ACM Multimedia 2025</a>!</p>
                    
                    <p>As human-computer interaction evolves, emotional intelligence and empathy are becoming essential capabilities of intelligent systems. The CogMAEC Workshop (<em class="accent-text">Cognition-oriented Multimodal Affective and Empathetic Computing</em>) aims to push the boundaries of traditional affective computing by exploring the next frontier: <strong>cognitive emotional understanding</strong>.</p>
                    
                    <p>While previous work in multimodal affective computing has focused on recognizing basic emotions from facial expressions, speech, and text, this workshop sets its sights on deeper challenges — understanding the <strong>"why"</strong> behind emotions, reasoning over context, and simulating human-like empathetic responses. With the recent advances in <strong>Multimodal Large Language Models (MLLMs)</strong>, the time is ripe to rethink how machines perceive, reason, and respond to human emotions.</p>
                    
                    <p>CogMAEC'25 brings together researchers and practitioners working on:</p>
                    <ul class="research-areas">
                        <li>Traditional Multimodal Affective Computing</li>
                        <li>MLLM-based Multimodal Affective Computing</li>
                        <li>Cognition-oriented Multimodal Affective Computing</li>
                    </ul>
                    
                    <p>The workshop will cover both traditional multimodal emotion recognition techniques and cutting-edge cognition-driven methodologies. We aim to foster meaningful discussion and collaboration at the intersection of <strong>affective computing</strong>, <strong>cognitive modeling</strong>, and <strong>multimodal AI</strong>.</p>
                    
                    <p class="highlight-text accent-text">Join us as we collectively reimagine what emotional AI can become — not just smarter, but more human.</p>
                    
                    <p class="note">All workshop details, schedules, and updates can be found on our <a href="https://CogMAEC.github.io/MM2025" target="_blank">website</a>.</p>
                </div>
            </div>
        </section>

        <section id="schedule" class="schedule-section">
            <div class="container">
                <h2>Schedule</h2>
                <div class="schedule-intro">
                    <p class="schedule-text">CogMAEC'25, co-organized with the MuSe workshop, runs in a hybrid format so that onsite and remote participants can engage together. The afternoon program combines invited <b>keynotes</b> with <b>oral</b> and <b>poster</b> presentations to spotlight cognition-oriented affective computing.</p>
                    <p class="schedule-meta">Monday, 27 October · 13:30&ndash;17:00 · Dublin Royal Convention Centre (<a href="https://www.google.com/maps/place/Dublin+Royal+Convention+Centre/@53.1607058,-6.7601398,8.59z/data=!4m6!3m5!1s0x48670d3bad4ddb5b:0x2d2ef37a3d00c938!8m2!3d53.3419947!4d-6.2683773!16s%2Fg%2F11hdz9rpy6?entry=ttu&amp;g_ep=EgoyMDI1MTAxMy4wIKXMDSoASAFQAw%3D%3D" target="_blank" class="schedule-map-link">Google Maps</a>) · Royal CC · Higgins&nbsp;2</p>
                </div>
                <div class="schedule-table-wrapper">
                    <table class="schedule-table">
                        <colgroup>
                            <col style="width: 12%;">
                            <col style="width: 64%;">
                            <col style="width: 24%;">
                        </colgroup>
                        <thead>
                            <tr>
                                <th>Time</th>
                                <th>Session (All times are in Dublin, Winter Time, UTC+0)</th>
                                <th>Presenter</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td class="time-slot">13:30&ndash;14:15</td>
                                <td>
                                    <div class="session-title keynote">Keynote Talk I: <span class="session-highlight">[TBD]</span></div>
                                </td>
                                <td class="presenter">Prof. Minlie Huang<br><span class="presenter-affiliation">Tsinghua University</span></td>
                            </tr>
                            <tr>
                                <td class="time-slot">14:15&ndash;15:00</td>
                                <td>
                                    <div class="session-title keynote">Keynote Talk II: <span class="session-highlight">[TBD]</span></div>
                                </td>
                                <td class="presenter">Prof. Soujanya Poria<br><span class="presenter-affiliation">Nanyang Technological University</span></td>
                            </tr>
                            <tr class="schedule-break">
                                <td colspan="3">
                                    Coffee Break &amp; Poster Session
                                </td>
                            </tr>
                            <tr>
                                <td class="time-slot">15:30&ndash;16:15</td>
                                <td>
                                    <details class="session-toggle keynote">
                                        <summary>Keynote Talk III: <span class="session-highlight">Diffusion beats autoregressive in data-constrained settings.</span></summary>
                                        <div class="session-toggle-content">
                                            <p><b>Abstract:</b> Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings—where training involves repeated passes over limited data—and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We find new scaling laws for diffusion models and derive a closedform expression for the critical compute threshold at which diffusion begins to outperform AR. Finally, we explain why diffusion models excel in this regime: their randomized masking objective implicitly trains over a rich distribution of token orderings, acting as an implicit data augmentation that AR’s fixed left-toright factorization lacks. Our results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm.</p>
                                        </div>
                                    </details>
                                </td>
                                <td class="presenter">Dr. Amir Zadeh<br><span class="presenter-affiliation">Lambda</span></td>
                            </tr>
                            <tr>
                                <td class="time-slot">16:15&ndash;16:30</td>
                                <td>
                                    <div class="session-title oral">Oral I: <span class="session-highlight">[TBD]</span></div>
                                </td>
                                <td class="presenter">Om Dabral</td>
                            </tr>
                            <tr>
                                <td class="time-slot">16:30&ndash;16:45</td>
                                <td>
                                    <div class="session-title oral">Oral II: <span class="session-highlight">Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via EEG-guided Audiovisual Generation</span></div>
                                </td>
                                <td class="presenter">Joonwoo Kwon</td>
                            </tr>
                            <tr>
                                <td class="time-slot">16:45&ndash;17:00</td>
                                <td>
                                    <div class="session-title oral">Oral III: <span class="session-highlight">Leveraging Concept Annotations for Trustworthy Multimodal Video Interpretation through Modality Specialization</span></div>
                                </td>
                                <td class="presenter">Elisa Ancarani</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </section>


        <section id="accepted-papers" class="papers-section">
            <div class="container">
                <h2>Accepted Papers</h2>
                <div class="papers-table-wrapper">
                    <table class="papers-table">
                        <thead>
                            <tr>
                                <th>#</th>
                                <th>Title</th>
                                <th>Authors</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>1.</td>
                                <td class="paper-title">PetChat: An Emotion-Aware Pet Communication System Powered by LLMs and Wearable Devices</td>
                                <td>Ziqiao Zhu, Jiachun Du, Kejun Zhang, Jingyuan Li</td>
                            </tr>
                            <tr>
                                <td>2.</td>
                                <td class="paper-title">Disentangled Representation Learning via Transformer with Graph Attention Fusion for Depression Detection</td>
                                <td>Luntian Mou, Siqi Zhen, Shasha Mao, Nan Ma</td>
                            </tr>
                            <tr>
                                <td>3.</td>
                                <td class="paper-title">Commanding the Debate Stage: Multimodal Emotion Analysis of Trump's Storytelling Strategies in the 2016 Presidential Debates</td>
                                <td>Xiuchuan Ding, Qiqi Gao</td>
                            </tr>
                            <tr>
                                <td>4.</td>
                                <td class="paper-title">Emotion Understanding under Naturalistic Stimuli via Neural Encoding and Decoding</td>
                                <td>Guandong Pan, Shaoting Tang, Zhiming Zheng, Yang Yangqian, Xin Wang, Liu Longzhao, Shi Chen</td>
                            </tr>
                            <tr>
                                <td>5.</td>
                                <td class="paper-title">Talk to Me, Like Me: Modular Personalization of Emotional AI via Behavioral Metadata, Fine-Tuning, RAG, Prompts, and Agentic Reasoning</td>
                                <td>Om Dabral, Jaspreet Singh, Hardik Sharma, Bagesh Kumar</td>
                            </tr>
                            <tr>
                                <td>6.</td>
                                <td class="paper-title">Multimodal Trait and Emotion Recognition via Agentic AI: An End-to-End Pipeline</td>
                                <td>Om Dabral, Swayam Bansal, Mridul Maheshwari, Hardik Sharma, Jaspreet Singh, Bagesh Kumar</td>
                            </tr>
                            <tr>
                                <td>7.</td>
                                <td class="paper-title">Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via EEG-guided Audiovisual Generation</td>
                                <td>Joonwoo Kwon, Heehwan Wang, Jinwoo Lee, Sooyoung Kim, Shinjae Yoo, Yuewei Lin, Jiook Cha</td>
                            </tr>
                            <tr>
                                <td>8.</td>
                                <td class="paper-title">Fine-grained Structured Multimodal Textural Representation for Natural Human-Computer Conversation</td>
                                <td>Yansong Liu, Yuxin Lin, Yinglin Zheng, Wangzheng Shi, Mingyi Xu, Yuhang Lin, Xinqi Cai, Dong Chen, Ming Zeng</td>
                            </tr>
                            <tr>
                                <td>9.</td>
                                <td class="paper-title">Leveraging Concept Annotations for Trustworthy Multimodal Video Interpretation through Modality Specialization</td>
                                <td>Elisa Ancarani, Julie Tores, Rémy Sun, Lucile Sassatelli, Hui-Yin Wu, Frederic Precioso</td>
                            </tr>
                            <tr>
                                <td>10.</td>
                                <td class="paper-title">Unveiling Genuine Emotions: Integrating Micro-Expressions and Physiological Signals for Enhanced Emotion Recognition</td>
                                <td>Chuang Ma</td>
                            </tr>
                            <tr>
                                <td>11.</td>
                                <td class="paper-title">A Transformer-Based Multimodal Framework for Hidden Emotion Recognition through Micro-Expression and EEG Fusion</td>
                                <td>Chuang Ma</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </section>


        <section id="call-for-paper">
            <div class="container">
                <h2>Call for Papers</h2>
                <div class="about-content">
                    <p>We invite contributions in three categories:</p>

                    <p><strong>1. Novel Position or Perspective Papers</strong> (4–8 pages, excl. references, archival). Forward-looking works that propose new ideas, conceptual frameworks, or identify open challenges aligned with the workshop themes. Accepted papers will appear in the CogMAEC 2025 Proceedings (co-located with MM '25).</p>

                    <p><strong>2. Non-archival Featured Papers</strong> (title + abstract + original manuscript). Influential papers already published in top venues, or well-curated summaries of substantial prior work. These submissions are presentation-only and will not be included in the proceedings.</p>

                    <p><strong>3. Demonstration Papers</strong> (≤ 2 pages, excl. references, archival). Short papers describing prototypes, tools, or systems that showcase practical implementations or evaluation methodologies. Accepted demos will be published in the CogMAEC 2025 Proceedings (co-located with MM '25).</p>

                    <p>All accepted submissions will be invited to present their work at the workshop.</p>
                    
                    <div class="section-divider"></div>
                    
                    <p>The workshop welcomes submissions on the following topics (but not limited to):</p>
                    
                    <div class="research-areas-container">
                        <div class="research-area-column">
                            <p><strong>1) Traditional Multimodal Affective Computing</strong></p>
                            <ul class="research-areas">
                                <li>Facial Expression Recognition</li>
                                <li>Speech Emotion Recognition</li>
                                <li>Audio-visual Emotion Recognition</li>
                                <li>Body Gesture Emotion Detection</li>
                                <li>Micro-expression Recognition</li>
                                <li>Multimodal Sentiment Analysis</li>
                                <li>Multimodal Emotion Recognition in Conversation</li>
                                <li>Multimodal Stance Detection</li>
                                <li>Multimodal Emotion Analysis in Memes</li>
                                <li>Multimodal Sarcasm and Irony Detection</li>
                                <li>Cross-cultural Emotion Recognition</li>
                                <li>Physiological Signal-based Emotion Recognition</li>
                                <li>Emotion-aware Dialogue Generation</li>
                                <li>Emotional Speech Synthesis</li>
                                <li>Multimodal Affective Storytelling</li>
                                <li>Affective Music Generation</li>
                                <li>Affective Facial Animation</li>
                                <li>Emotion-controlled Avatar Generation</li>
                            </ul>
                        </div>

                        <div class="research-area-column">
                            <p><strong>2) MLLM-based Multimodal Affective Computing</strong></p>
                            <ul class="research-areas">
                                <li>Few-shot Emotion Recognition</li>
                                <li>Multimodal Emotion Reasoning</li>
                                <li>Multimodal Affective Hallucination Mitigation</li>
                                <li>Emotion-aware Self-supervised Representation Learning</li>
                                <li>Multimodal Affective In-context Learning</li>
                                <li>Affective Instruction Tuning for MLLMs</li>
                                <li>Multimodal Feature Extraction and Fusion</li>
                                <li>Cross-modal Affective Alignment</li>
                                <li>Cross-domain Affective Transfer Learning</li>
                                <li>Emotion-aware Visual Question Answering</li>
                                <li>Emotion-guided Text-to-Image/Video Generation</li>
                                <li>Multimodal Empathetic Dialogue Systems</li>
                                <li>Persona-driven Emotion-aware Conversational AI</li>
                            </ul>
                        </div>

                        <div class="research-area-column">
                            <p><strong>3) Cognition-oriented Multimodal Affective Computing</strong></p>
                            <ul class="research-areas">
                                <li>Multimodal Implicit Sentiment Analysis</li>
                                <li>Multimodal Emotion Cause Analysis in Conversations</li>
                                <li>Multimodal Aspect-based Sentiment Analysis</li>
                                <li>Neuro-symbolic Reasoning for Emotion Understanding</li>
                                <li>Theory of Mind-based Empathy Modeling</li>
                                <li>Cognitive Load and Affect Interaction Modeling</li>
                                <li>Cross-modal Cognitive Bias Detection</li>
                            </ul>
                        </div>
                    </div>

                    <style>
                        .research-areas-container {
                            display: flex;
                            justify-content: space-between;
                            gap: 20px;
                            margin: 20px 0;
                            width: 100%;
                        }
                        
                        .research-area-column {
                            flex: 1;
                            min-width: 0;
                            background: rgba(255, 255, 255, 0.05);
                            padding: 20px;
                            border-radius: 10px;
                            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
                        }
                        
                        .research-area-column strong {
                            display: block;
                            margin-bottom: 15px;
                            color: var(--accent-color, #007bff);
                            font-size: 1.1em;
                            word-wrap: break-word;
                        }
                        
                        .research-areas {
                            list-style-type: none;
                            padding-left: 0;
                            margin: 0;
                        }
                        
                        .research-areas li {
                            margin-bottom: 10px;
                            padding-left: 20px;
                            position: relative;
                            word-wrap: break-word;
                        }
                        
                        .research-areas li:before {
                            content: "•";
                            position: absolute;
                            left: 0;
                            color: var(--accent-color, #007bff);
                        }
                        
                        @media (max-width: 768px) {
                            .research-areas-container {
                                flex-direction: column;
                            }
                            
                            .research-area-column {
                                margin-bottom: 20px;
                                width: 100%;
                            }
                        }
                    </style>
                </div>
            </div>
        </section>

        <section id="timeline">
            <div class="container">
                <h2>Important Dates</h2>
                <div class="timeline">
                    <div class="timeline-item">
                        <h3>Workshop Date</h3>
                        <p class="date" data-date="2025-10-28T23:59:00-1200">October 27-28, 2025 (AoE)</p>
                    </div>
                    <div class="timeline-item">
                        <h3>Camera Ready</h3>
                        <p class="date" data-date="2025-08-13T23:59:00-1200">August 13, 2025 (AoE)</p>
                    </div>
                    <div class="timeline-item">
                        <h3>Paper Notification</h3>
                        <p class="date" data-date="2025-08-05T23:59:00-1200">August 5, 2025 (AoE)</p>
                    </div>
                    <div class="timeline-item">
                        <h3>Paper Submission Deadline</h3>
                        <p class="date" data-date="2025-06-30T23:59:00-1200">June 30, 2025 (AoE)</p>
                    </div>
                    <div class="timeline-item">
                        <h3>Paper Submission Start</h3>
                        <p class="date" data-date="2025-04-15T23:59:00-1200">April 15, 2025 (AoE)</p>
                    </div>
                    <div class="timeline-item">
                        <h3>Website Preparation</h3>
                        <p class="date" data-date="2025-03-30T23:59:00-1200">March 30, 2025 (AoE)</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="submission">
            <div class="container">
                <!-- <h2>Submission Guidelines</h2> -->
                <h2>Submission Guidelines</h2>
                <div class="about-content">
                    <p>All submissions must be written in English and follow the current ACM two-column conference format. Page limits are inclusive of all content, including figures and appendices. Submissions must be anonymized by the authors for review.</p>
                    
                    <p>Authors should use the appropriate ACM templates: the "sigconf" LaTeX template or the Interim Word Template, both available on the <a href="https://www.acm.org/publications/proceedings-template" target="_blank" class="theme-link">ACM Proceedings Template page</a>. Alternatively, authors can prepare their submissions using <a href="https://www.overleaf.com/gallery/tagged/acm-official#.WOuOk2e1taQ" target="_blank" class="theme-link">Overleaf's official ACM templates</a>.</p>

                    <!-- <p>For submission and review of manuscript of latex format, you could use the following documentclass command:  -->
                    <!-- <em>\documentclass[sigconf, screen, review, anonymous]{acmart}</em></p> -->
                     <!-- <p>Use <code>\documentclass[sigconf, screen, review, anonymous]{acmart}</code> to prepare your LaTeX manuscript for submission and review.</p> -->
                     <!-- <p>To prepare your manuscript in LaTeX format for submission and review, please use the following document class: <em>\documentclass[sigconf, screen, review, anonymous]{acmart}</em>.</p> -->
                     <p>Please use <em>\documentclass[sigconf, screen, review, anonymous]{acmart}</em> when preparing your LaTeX manuscript for submission and review.</p>



                    
                    <div class="submission-button-container">
                        <a href="https://openreview.net/group?id=acmmm.org/ACMMM/2025/Workshop/CogMAEC" class="submission-button" target="_blank">
                            <i class="fas fa-paper-plane"></i>
                            Submission Site
                        </a>
                    </div>
                </div>
            </div>
        </section>

   <section id="invited-speaker">
            <div class="container">
                <h2>Invited Speakers</h2>
                <div class="about-content">
                    <p style="text-align: center; margin-bottom: 30px;">We have invited the following renowned scholars in the field of cognition and affective computing</p>
                </div>
                <div class="speakers">
                    <div class="speaker">
                        <img src="assets/mlhuang.jpg" alt="Prof. Minlie Huang">
                        <h3><a href="https://coai.cs.tsinghua.edu.cn/hml" target="_blank">Prof. Minlie Huang</a></h3>
                        <p class="institution">Tsinghua University</p>
                    </div>
                    <div class="speaker">
                        <img src="assets/poria.png" alt="Prof. Soujanya Poria">
                        <h3><a href="https://soujanyaporia.github.io/" target="_blank">Prof. Soujanya Poria</a></h3>
                        <p class="institution">Nanyang Technological University</p>
                    </div>
                    <div class="speaker">
                        <img class="compact" src="assets/amir.jpeg" alt="Dr. Amir Zadeh">
                        <h3><a href="https://scholar.google.com/citations?user=MQFngiMAAAAJ" target="_blank">Dr. Amir Zadeh</a></h3>
                        <p class="institution">Lambda</p>
                    </div>
                </div>
            </div>
        </section>
        

        <section id="organizers">
            <div class="container">
                <h2>Organizers</h2>
                <div class="organizers">
                    <div class="organizer">
                        <img src="assets/haofei.jpg" alt="Hao Fei">
                        <h3><a href="https://haofei.vip/" target="_blank">Hao Fei</a></h3>
                        <p class="institution">National University of Singapore</p>
                    </div>
                    <div class="organizer">
                        <img src="assets/libobo.png" alt="Bobo Li">
                        <h3><a href="https://www.libobo.site/" target="_blank">Bobo Li</a></h3>
                        <p class="institution">National University of Singapore</p>
                    </div>
                    <div class="organizer">
                        <img src="assets/mengluo.jpg" alt="Meng Luo">
                        <h3><a href="https://eurekaleo.github.io/" target="_blank">Meng Luo</a></h3>
                        <p class="institution">National University of Singapore</p>
                    </div>
                    <div class="organizer">
                        <img src="assets/qianliu.jpg" alt="Qian Liu">
                        <h3><a href="https://profiles.auckland.ac.nz/liu-qian" target="_blank">Qian Liu</a></h3>
                        <p class="institution">University of Auckland</p>
                    </div>
                    <div class="organizer">
                        <img src="assets/liziliao.jpg" alt="Lizi Liao">
                        <h3><a href="https://liziliao.github.io/" target="_blank">Lizi Liao</a></h3>
                        <p class="institution">Singapore Management University</p>
                    </div>
                    <div class="organizer">
                        <img src="assets/feili.jpg" alt="Fei Li">
                        <h3><a href="https://jszy.whu.edu.cn/lifei10/zh_CN/index.htm" target="_blank">Fei Li</a></h3>
                        <p class="institution">Wuhan University</p>
                    </div>
                    <div class="organizer">
                        <img src="assets/mingzhang.png" alt="Min Zhang">
                        <h3><a href="https://faculty.hitsz.edu.cn/MinZhang" target="_blank">Min Zhang</a></h3>
                        <p class="institution">Harbin Institute of Technology (Shenzhen)</p>
                    </div>
                    <div class="organizer">
                        <img src="assets/schuller.jpg" alt="Björn W. Schuller">
                        <h3><a href="http://www.schuller.one/" target="_blank">Björn W. Schuller</a></h3>
                        <p class="institution">Imperial College London</p>
                    </div>
                    <div class="organizer">
                        <img src="assets/mongli.jpg" alt="Mong-Li Lee">
                        <h3><a href="https://www.comp.nus.edu.sg/~leeml/" target="_blank">Mong-Li Lee</a></h3>
                        <p class="institution">National University of Singapore</p>
                    </div>
                    <div class="organizer">
                        <img src="assets/erik.jpeg" alt="Erik Cambria">
                        <h3><a href="https://dr.ntu.edu.sg/cris/rp/rp00927" target="_blank">Erik Cambria</a></h3>
                        <p class="institution">Nanyang Technological University</p>
                    </div>
                </div>
            </div>
        </section>
     

        <section id="contact">
            <div class="container">
                <h2>Contact</h2>
                <div class="about-content">
                    <p>For any questions about the workshop, please contact us through:</p>
                    <div class="contact-info">
                        <p><strong>Email:</strong> <button onclick="showEmail()" class="email-btn" id="emailBtn">Show Email</button><span id="emailText" class="email-text"></span></p>
                        <p><strong>Google Group:</strong> <a href="https://groups.google.com/g/cogmaec" target="_blank">https://groups.google.com/g/cogmaec</a></p>
                    </div>
                </div>
            </div>
        </section>

        <footer>
            <div class="container">
                <div class="footer-bottom">
                    <p>&copy; 2025 CogMAEC Workshop. All rights reserved.</p>
                </div>
            </div>
        </footer>
    </div>

    <script>
        // 日期判断函数
        function checkDates() {
            const dates = document.querySelectorAll('.date');
            const now = new Date();
            
            dates.forEach(dateElement => {
                const dateStr = dateElement.getAttribute('data-date');
                // 直接使用完整的ISO 8601格式的日期字符串
                const date = new Date(dateStr);
                
                if (now > date) {
                    dateElement.classList.add('passed');
                }
            });
        }

        // 页面加载时检查日期
        document.addEventListener('DOMContentLoaded', checkDates);
        
        // 每分钟更新一次日期状态
        setInterval(checkDates, 60000);

        // 汉堡菜单功能
        document.querySelector('.hamburger-menu').addEventListener('click', function() {
            document.querySelector('.nav-items').classList.toggle('active');
            this.classList.toggle('active');
        });

        // 点击链接后关闭菜单
        document.querySelectorAll('.nav-items a').forEach(item => {
            item.addEventListener('click', function() {
                document.querySelector('.nav-items').classList.remove('active');
                document.querySelector('.hamburger-menu').classList.remove('active');
            });
        });

        // 平滑滚动
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });

        // 导航高亮和滚动效果
        window.addEventListener('scroll', function() {
            const navbar = document.querySelector('.navbar');
            const sections = document.querySelectorAll('section');
            const navItems = document.querySelectorAll('.nav-items a');
            
            // 添加导航栏滚动效果
            if (window.scrollY > 50) {
                navbar.classList.add('scrolled');
            } else {
                navbar.classList.remove('scrolled');
            }
            
            // 高亮当前部分
            let current = '';
            
            const scrollPosition = window.pageYOffset + window.innerHeight;
            const documentHeight = document.documentElement.scrollHeight;
            const bottomThreshold = 100; // 距离底部100px就算到底部

            // 如果接近底部，直接高亮contact
            if (documentHeight - scrollPosition < bottomThreshold) {
                current = 'contact';
            } else {
                // 否则正常检测各个section
                sections.forEach(section => {
                    const sectionTop = section.offsetTop;
                    if (pageYOffset >= sectionTop - 150) {
                        current = section.getAttribute('id');
                    }
                });
            }

            navItems.forEach(item => {
                item.classList.remove('active');
                if (item.getAttribute('href') === `#${current}`) {
                    item.classList.add('active');
                }
            });
        });

        function showEmail() {
            const btn = document.getElementById('emailBtn');
            const emailText = document.getElementById('emailText');
            const parts = ['cogmaec', '@', 'googlegroups.com'];
            emailText.textContent = parts.join('');
            emailText.classList.add('revealed');
            btn.style.display = 'none';
        }
    </script>
</body>
</html> 
